{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f8d46c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class HeadlineNews:\n",
    "    def __init__(self, actual_ctr=[0.1, 0.2]):\n",
    "        self.num_news = len(actual_ctr)\n",
    "        self.actual_ctr = actual_ctr\n",
    "    \n",
    "    def place(self, selected_news_index):\n",
    "        if selected_news_index > self.num_news:\n",
    "            raise Exception(f'the selected index {selected_news_index} is invalid')\n",
    "        \n",
    "        ctr = self.actual_ctr[selected_news_index]\n",
    "        clicked = [0, 1]\n",
    "        reward = random.choices(clicked, weights = [1 - ctr, ctr], k=1)\n",
    "\n",
    "        return reward[0]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "8f9b532c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 1 for the action 1\n",
      "reward is 1 for the action 1\n",
      "reward is 1 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 1 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 1 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 1 for the action 1\n",
      "reward is 1 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 1 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 1 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 1 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 1 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 1 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 1 for the action 1\n",
      "reward is 0 for the action 1\n",
      "reward is 0 for the action 1\n",
      "average reward value is 0.13\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "hn = HeadlineNews(actual_ctr=[0.3, 0.1])\n",
    "rewards = []\n",
    "for i in range(100):\n",
    "    action = 1\n",
    "    reward = hn.place(action)\n",
    "    rewards.append(reward)\n",
    "    print(f'reward is {reward} for the action {action}')\n",
    "    \n",
    "print(f'average reward value is {statistics.mean(rewards)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "0eab7d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, epsilon, num_action, verbose=False):\n",
    "        self.epsilon = epsilon\n",
    "        self.num_action = num_action\n",
    "        self.placement_count = np.ones(num_action)\n",
    "        self.click_count = np.zeros(num_action)\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def get_action(self):\n",
    "        if random.uniform(0, 1) <= self.epsilon:\n",
    "            action = random.randint(0, self.num_action-1)\n",
    "            if self.verbose:\n",
    "                print(f'random selection:{action}')\n",
    "        else:\n",
    "            ctr = self.get_ctr()\n",
    "            \n",
    "            idx_max = np.argwhere(ctr == np.amax(ctr))\n",
    "            idx_max_list = idx_max.flatten().tolist()\n",
    "            \n",
    "            action = random.choice(idx_max_list)\n",
    "            \n",
    "            if self.verbose:\n",
    "                print(f'select best known action:{action}')\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def get_ctr(self):\n",
    "        return np.divide(self.click_count, self.placement_count)\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        self.placement_count[action] = self.placement_count[action] + 1\n",
    "        self.click_count[action] = self.click_count[action] + reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "1ccdf855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "select best known action:1\n",
      "random selection:0\n",
      "random selection:0\n",
      "select best known action:1\n",
      "random selection:0\n",
      "random selection:0\n",
      "random selection:0\n",
      "select best known action:1\n",
      "random selection:0\n",
      "select best known action:1\n"
     ]
    }
   ],
   "source": [
    "eg = EpsilonGreedy(epsilon = 0.5, num_action = 2, verbose=True)\n",
    "# you will observe that about half of the time, action is randomly selected, \n",
    "\n",
    "for i in range(10):\n",
    "    eg.get_action()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "c70a0a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observed CTR is [0.99009901 0.        ]\n",
      "select best known action:0\n",
      "random selection:0\n",
      "select best known action:0\n",
      "random selection:1\n",
      "select best known action:0\n",
      "random selection:1\n",
      "select best known action:0\n",
      "random selection:0\n",
      "random selection:1\n",
      "select best known action:0\n"
     ]
    }
   ],
   "source": [
    "# let's build a case that the first news (with action index 0) always get clicked for 100 times of placement\n",
    "eg = EpsilonGreedy(epsilon = 0.5, num_action = 2, verbose=True)\n",
    "\n",
    "for i in range(100):\n",
    "    eg.update(0, 1)\n",
    "\n",
    "\n",
    "print(f'observed CTR is {eg.get_ctr()}')\n",
    "\n",
    "# you will noticed that for the half of the time to chose ation base on the best observed CTR, \n",
    "# the first news (with action index 0) are always slected\n",
    "for i in range(10):\n",
    "    eg.get_action()\n",
    "    #print(eg.get_action())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "9a6a4d2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([101.,   1.])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg.placement_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a1da96cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([100.,   0.])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg.click_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "19567e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.99009901, 0.        ])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eg.get_ctr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "b868b82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the episode0, total rewards is 187\n",
      "for the episode1, total rewards is 202\n",
      "for the episode2, total rewards is 211\n",
      "for the episode3, total rewards is 215\n",
      "for the episode4, total rewards is 193\n",
      "for the episode5, total rewards is 212\n",
      "for the episode6, total rewards is 176\n",
      "for the episode7, total rewards is 220\n",
      "for the episode8, total rewards is 195\n",
      "for the episode9, total rewards is 194\n",
      "for the episode10, total rewards is 177\n",
      "for the episode11, total rewards is 214\n",
      "for the episode12, total rewards is 179\n",
      "for the episode13, total rewards is 201\n",
      "for the episode14, total rewards is 184\n",
      "for the episode15, total rewards is 184\n",
      "for the episode16, total rewards is 191\n",
      "for the episode17, total rewards is 217\n",
      "for the episode18, total rewards is 214\n",
      "for the episode19, total rewards is 187\n",
      "for the episode20, total rewards is 201\n",
      "for the episode21, total rewards is 190\n",
      "for the episode22, total rewards is 187\n",
      "for the episode23, total rewards is 205\n",
      "for the episode24, total rewards is 210\n",
      "for the episode25, total rewards is 173\n",
      "for the episode26, total rewards is 192\n",
      "for the episode27, total rewards is 209\n",
      "for the episode28, total rewards is 199\n",
      "for the episode29, total rewards is 172\n",
      "for the episode30, total rewards is 190\n",
      "for the episode31, total rewards is 217\n",
      "for the episode32, total rewards is 194\n",
      "for the episode33, total rewards is 208\n",
      "for the episode34, total rewards is 201\n",
      "for the episode35, total rewards is 182\n",
      "for the episode36, total rewards is 173\n",
      "for the episode37, total rewards is 211\n",
      "for the episode38, total rewards is 196\n",
      "for the episode39, total rewards is 193\n",
      "for the episode40, total rewards is 195\n",
      "for the episode41, total rewards is 203\n",
      "for the episode42, total rewards is 201\n",
      "for the episode43, total rewards is 187\n",
      "for the episode44, total rewards is 189\n",
      "for the episode45, total rewards is 201\n",
      "for the episode46, total rewards is 179\n",
      "for the episode47, total rewards is 197\n",
      "for the episode48, total rewards is 205\n",
      "for the episode49, total rewards is 212\n",
      "for the episode50, total rewards is 198\n",
      "for the episode51, total rewards is 189\n",
      "for the episode52, total rewards is 180\n",
      "for the episode53, total rewards is 167\n",
      "for the episode54, total rewards is 174\n",
      "for the episode55, total rewards is 195\n",
      "for the episode56, total rewards is 182\n",
      "for the episode57, total rewards is 172\n",
      "for the episode58, total rewards is 172\n",
      "for the episode59, total rewards is 204\n",
      "for the episode60, total rewards is 192\n",
      "for the episode61, total rewards is 189\n",
      "for the episode62, total rewards is 200\n",
      "for the episode63, total rewards is 218\n",
      "for the episode64, total rewards is 191\n",
      "for the episode65, total rewards is 208\n",
      "for the episode66, total rewards is 192\n",
      "for the episode67, total rewards is 189\n",
      "for the episode68, total rewards is 190\n",
      "for the episode69, total rewards is 191\n",
      "for the episode70, total rewards is 192\n",
      "for the episode71, total rewards is 193\n",
      "for the episode72, total rewards is 224\n",
      "for the episode73, total rewards is 208\n",
      "for the episode74, total rewards is 202\n",
      "for the episode75, total rewards is 188\n",
      "for the episode76, total rewards is 206\n",
      "for the episode77, total rewards is 197\n",
      "for the episode78, total rewards is 179\n",
      "for the episode79, total rewards is 204\n",
      "for the episode80, total rewards is 203\n",
      "for the episode81, total rewards is 229\n",
      "for the episode82, total rewards is 207\n",
      "for the episode83, total rewards is 206\n",
      "for the episode84, total rewards is 192\n",
      "for the episode85, total rewards is 196\n",
      "for the episode86, total rewards is 194\n",
      "for the episode87, total rewards is 177\n",
      "for the episode88, total rewards is 187\n",
      "for the episode89, total rewards is 191\n",
      "for the episode90, total rewards is 202\n",
      "for the episode91, total rewards is 194\n",
      "for the episode92, total rewards is 190\n",
      "for the episode93, total rewards is 205\n",
      "for the episode94, total rewards is 209\n",
      "for the episode95, total rewards is 203\n",
      "for the episode96, total rewards is 187\n",
      "for the episode97, total rewards is 194\n",
      "for the episode98, total rewards is 198\n",
      "for the episode99, total rewards is 187\n",
      "mean value of cumulative rewards for 1000 steps is 195.62\n",
      "standard deviation of cumulative rewards for 1000 steps is 12.745195173083856\n"
     ]
    }
   ],
   "source": [
    "import statistics \n",
    "\n",
    "\n",
    "actual_ctr = [0.1, 0.2]\n",
    "hn = HeadlineNews(actual_ctr)\n",
    "\n",
    "epsilon = 0.1\n",
    "num_action = len(actual_ctr)\n",
    "\n",
    "eg = EpsilonGreedy(epsilon, num_action)\n",
    "\n",
    "num_episode = 100\n",
    "num_step = 1000\n",
    "\n",
    "ep_rewards = []\n",
    "\n",
    "verbose = False\n",
    "for episode in range(num_episode):\n",
    "    cumulative_reward = 0\n",
    "    for step in range(num_step):    \n",
    "        action = eg.get_action()\n",
    "        reward = hn.place(action)\n",
    "\n",
    "        cumulative_reward = cumulative_reward + reward\n",
    "\n",
    "        eg.update(action, reward)\n",
    "\n",
    "        if verbose and step % 100 == 0:\n",
    "            print('==================================')\n",
    "            print(f'at step {step}')\n",
    "            print(f'current ctr is: {eg.get_ctr()}')\n",
    "            print(f'placement_count is: {eg.placement_count}')\n",
    "            print(f'click_count is: {eg.click_count}')\n",
    "\n",
    "    print(f'for the episode {episode}, total rewards is {cumulative_reward}')\n",
    "    ep_rewards.append(cumulative_reward)\n",
    "\n",
    "print(f'mean value of cumulative rewards for 1000 steps is {statistics.mean(ep_rewards)}')\n",
    "print(f'standard deviation of cumulative rewards for 1000 steps is {statistics.stdev(ep_rewards)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "a1d5051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class RandomSelection:\n",
    "    def __init__(self, num_action, verbose=False):\n",
    "        self.num_action = num_action\n",
    "        self.placement_count = np.ones(num_action)\n",
    "        self.click_count = np.zeros(num_action)\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def get_action(self):\n",
    "        action = random.randint(0, self.num_action-1)\n",
    "        return action\n",
    "    \n",
    "    def update(self, action, reward):\n",
    "        self.placement_count[action] = self.placement_count[action] + 1\n",
    "        self.click_count[action] = self.click_count[action] + reward\n",
    "        \n",
    "    def get_ctr(self):\n",
    "        return np.divide(self.click_count, self.placement_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "85063a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the episode 0, total rewards is 147\n",
      "for the episode 1, total rewards is 171\n",
      "for the episode 2, total rewards is 158\n",
      "for the episode 3, total rewards is 161\n",
      "for the episode 4, total rewards is 171\n",
      "for the episode 5, total rewards is 143\n",
      "for the episode 6, total rewards is 158\n",
      "for the episode 7, total rewards is 133\n",
      "for the episode 8, total rewards is 143\n",
      "for the episode 9, total rewards is 136\n",
      "for the episode 10, total rewards is 152\n",
      "for the episode 11, total rewards is 155\n",
      "for the episode 12, total rewards is 136\n",
      "for the episode 13, total rewards is 148\n",
      "for the episode 14, total rewards is 141\n",
      "for the episode 15, total rewards is 158\n",
      "for the episode 16, total rewards is 145\n",
      "for the episode 17, total rewards is 155\n",
      "for the episode 18, total rewards is 141\n",
      "for the episode 19, total rewards is 150\n",
      "for the episode 20, total rewards is 139\n",
      "for the episode 21, total rewards is 156\n",
      "for the episode 22, total rewards is 146\n",
      "for the episode 23, total rewards is 150\n",
      "for the episode 24, total rewards is 163\n",
      "for the episode 25, total rewards is 178\n",
      "for the episode 26, total rewards is 137\n",
      "for the episode 27, total rewards is 167\n",
      "for the episode 28, total rewards is 144\n",
      "for the episode 29, total rewards is 140\n",
      "for the episode 30, total rewards is 133\n",
      "for the episode 31, total rewards is 160\n",
      "for the episode 32, total rewards is 140\n",
      "for the episode 33, total rewards is 140\n",
      "for the episode 34, total rewards is 143\n",
      "for the episode 35, total rewards is 142\n",
      "for the episode 36, total rewards is 147\n",
      "for the episode 37, total rewards is 161\n",
      "for the episode 38, total rewards is 153\n",
      "for the episode 39, total rewards is 139\n",
      "for the episode 40, total rewards is 130\n",
      "for the episode 41, total rewards is 136\n",
      "for the episode 42, total rewards is 149\n",
      "for the episode 43, total rewards is 138\n",
      "for the episode 44, total rewards is 156\n",
      "for the episode 45, total rewards is 150\n",
      "for the episode 46, total rewards is 160\n",
      "for the episode 47, total rewards is 168\n",
      "for the episode 48, total rewards is 155\n",
      "for the episode 49, total rewards is 164\n",
      "for the episode 50, total rewards is 160\n",
      "for the episode 51, total rewards is 148\n",
      "for the episode 52, total rewards is 147\n",
      "for the episode 53, total rewards is 168\n",
      "for the episode 54, total rewards is 151\n",
      "for the episode 55, total rewards is 158\n",
      "for the episode 56, total rewards is 146\n",
      "for the episode 57, total rewards is 166\n",
      "for the episode 58, total rewards is 154\n",
      "for the episode 59, total rewards is 146\n",
      "for the episode 60, total rewards is 149\n",
      "for the episode 61, total rewards is 146\n",
      "for the episode 62, total rewards is 132\n",
      "for the episode 63, total rewards is 136\n",
      "for the episode 64, total rewards is 170\n",
      "for the episode 65, total rewards is 155\n",
      "for the episode 66, total rewards is 178\n",
      "for the episode 67, total rewards is 145\n",
      "for the episode 68, total rewards is 163\n",
      "for the episode 69, total rewards is 143\n",
      "for the episode 70, total rewards is 139\n",
      "for the episode 71, total rewards is 132\n",
      "for the episode 72, total rewards is 160\n",
      "for the episode 73, total rewards is 144\n",
      "for the episode 74, total rewards is 159\n",
      "for the episode 75, total rewards is 153\n",
      "for the episode 76, total rewards is 154\n",
      "for the episode 77, total rewards is 153\n",
      "for the episode 78, total rewards is 142\n",
      "for the episode 79, total rewards is 146\n",
      "for the episode 80, total rewards is 156\n",
      "for the episode 81, total rewards is 153\n",
      "for the episode 82, total rewards is 140\n",
      "for the episode 83, total rewards is 148\n",
      "for the episode 84, total rewards is 143\n",
      "for the episode 85, total rewards is 148\n",
      "for the episode 86, total rewards is 158\n",
      "for the episode 87, total rewards is 143\n",
      "for the episode 88, total rewards is 173\n",
      "for the episode 89, total rewards is 154\n",
      "for the episode 90, total rewards is 148\n",
      "for the episode 91, total rewards is 155\n",
      "for the episode 92, total rewards is 146\n",
      "for the episode 93, total rewards is 141\n",
      "for the episode 94, total rewards is 127\n",
      "for the episode 95, total rewards is 126\n",
      "for the episode 96, total rewards is 132\n",
      "for the episode 97, total rewards is 134\n",
      "for the episode 98, total rewards is 137\n",
      "for the episode 99, total rewards is 150\n",
      "mean value of cumulative rewards for 1000 steps is 149.41\n",
      "standard deviation of cumulative rewards for 1000 steps is 11.329852406609522\n"
     ]
    }
   ],
   "source": [
    "actual_ctr = [0.1, 0.2]\n",
    "hn = HeadlineNews(actual_ctr)\n",
    "\n",
    "num_action = len(actual_ctr)\n",
    "\n",
    "rs = RandomSelection(num_action)\n",
    "\n",
    "num_episode = 100\n",
    "num_step = 1000\n",
    "\n",
    "rs_rewards = []\n",
    "\n",
    "verbose = False\n",
    "for episode in range(num_episode):\n",
    "    cumulative_reward = 0\n",
    "    for step in range(num_step):    \n",
    "        action = rs.get_action()\n",
    "        reward = hn.place(action)\n",
    "\n",
    "        cumulative_reward = cumulative_reward + reward\n",
    "\n",
    "        rs.update(action, reward)\n",
    "\n",
    "        if verbose and step % 100 == 0:\n",
    "            print('==================================')\n",
    "            print(f'at step {step}')\n",
    "            print(f'current ctr is: {rs.get_ctr()}')\n",
    "            print(f'placement_count is: {rs.placement_count}')\n",
    "            print(f'click_count is: {rs.click_count}')\n",
    "\n",
    "    print(f'for the episode {episode}, total rewards is {cumulative_reward}')\n",
    "    rs_rewards.append(cumulative_reward)\n",
    "\n",
    "print(f'mean value of cumulative rewards for 1000 steps is {statistics.mean(rs_rewards)}')\n",
    "print(f'standard deviation of cumulative rewards for 1000 steps is {statistics.stdev(rs_rewards)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "f0dd2176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2358974358974359"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(195-149)/195"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
